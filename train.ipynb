{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab04043-351b-4f9e-aceb-e8527c88510a",
      "metadata": {
        "id": "5ab04043-351b-4f9e-aceb-e8527c88510a",
        "outputId": "b330a7f3-2642-43ef-9b3e-bfccccce5eaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-g8ov9yo6\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-g8ov9yo6\n",
            "  Resolved https://github.com/openai/CLIP.git to commit b46f5ac7587d2e1862f8b7b1573179d80dcdd620\n",
            "Requirement already satisfied: ftfy in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from clip==1.0) (2022.4.24)\n",
            "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from clip==1.0) (1.11.0)\n",
            "Requirement already satisfied: torchvision in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from clip==1.0) (0.12.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch->clip==1.0) (4.2.0)\n",
            "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from torchvision->clip==1.0) (9.1.1)\n",
            "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (1.26.9)\n",
            "Requirement already satisfied: transformers in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (4.19.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (2022.4.24)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install transformers\n",
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from typing import Tuple, List, Union, Optional\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "#from google.colab import files\n",
        "import skimage.io as io\n",
        "import PIL.Image\n",
        "from IPython.display import Image "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "456117a8-9bff-458b-b2c7-e2a42a5c6170",
      "metadata": {
        "id": "456117a8-9bff-458b-b2c7-e2a42a5c6170",
        "outputId": "3fcd6e9d-124a-49a4-ddc3-b025afc2fc24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data size is 158915\n",
            "Train both prefix and GPT\n",
            ">>> Training epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "prefix:  41%|████████████████████████▍                                   | 1621/3972 [42:02<59:42,  1.52s/it, loss=2.86]"
          ]
        }
      ],
      "source": [
        "#!pip install transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as nnf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from enum import Enum\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "from typing import Tuple, Optional, Union\n",
        "\n",
        "\n",
        "class MappingType(Enum):\n",
        "    MLP = 'mlp'\n",
        "    Transformer = 'transformer'\n",
        "\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.captions_tokens)\n",
        "\n",
        "    def pad_tokens(self, item: int):\n",
        "        tokens = self.captions_tokens[item]\n",
        "        padding = self.max_seq_len - tokens.shape[0]\n",
        "        if padding > 0:\n",
        "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
        "            self.captions_tokens[item] = tokens\n",
        "        elif padding < 0:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "            self.captions_tokens[item] = tokens\n",
        "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
        "        tokens[~mask] = 0\n",
        "        mask = mask.float()\n",
        "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n",
        "        return tokens, mask\n",
        "\n",
        "    def __getitem__(self, item: int) -> Tuple[torch.Tensor, ...]:\n",
        "        tokens, mask = self.pad_tokens(item)\n",
        "        prefix = self.prefixes[self.caption2embedding[item]]\n",
        "        if self.normalize_prefix:\n",
        "            prefix = prefix.float()\n",
        "            prefix = prefix / prefix.norm(2, -1)\n",
        "        return tokens, mask, prefix\n",
        "    \n",
        "    def __init__(self, data_path: str,  prefix_length: int, gpt2_type: str = \"gpt2\",\n",
        "                 normalize_prefix=False):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
        "        self.prefix_length = prefix_length\n",
        "        self.normalize_prefix = normalize_prefix\n",
        "        with open(data_path, 'rb') as f:\n",
        "            all_data = pickle.load(f)\n",
        "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
        "        sys.stdout.flush()\n",
        "        self.prefixes = all_data[\"clip_embedding\"]\n",
        "        captions_raw = all_data[\"captions\"]\n",
        "        image_ids = [caption[\"image_name\"] for caption in captions_raw]\n",
        "        captions = [caption['comment'] for caption in captions_raw]\n",
        "        if os.path.isfile(f\"{data_path[:-4]}_tokens.pkl\"):\n",
        "            with open(f\"{data_path[:-4]}_tokens.pkl\", 'rb') as f:\n",
        "                self.captions_tokens, self.caption2embedding, self.max_seq_len = pickle.load(f)\n",
        "        else:\n",
        "            self.captions_tokens = []\n",
        "            self.caption2embedding = []\n",
        "            max_seq_len = 0\n",
        "            for caption in captions_raw:\n",
        "                print(caption)\n",
        "                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption['comment']), dtype=torch.int64))\n",
        "                self.caption2embedding.append(caption[\"clip_embedding\"])\n",
        "                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n",
        "            # self.max_seq_len = max_seq_len\n",
        "            with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
        "                pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n",
        "        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n",
        "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class MlpTransformer(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n",
        "        super().__init__()\n",
        "        out_d = out_d if out_d is not None else in_dim\n",
        "        self.fc1 = nn.Linear(in_dim, h_dim)\n",
        "        self.act = act\n",
        "        self.fc2 = nn.Linear(h_dim, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim_self // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n",
        "        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n",
        "        self.project = nn.Linear(dim_self, dim_self)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        y = y if y is not None else x\n",
        "        b, n, c = x.shape\n",
        "        _, m, d = y.shape\n",
        "        # b n h dh\n",
        "        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n",
        "        # b m 2 h dh\n",
        "        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n",
        "        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n",
        "        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n",
        "        if mask is not None:\n",
        "            if mask.dim() == 2:\n",
        "                mask = mask.unsqueeze(1)\n",
        "            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n",
        "        attention = attention.softmax(dim=2)\n",
        "        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n",
        "        out = self.project(out)\n",
        "        return out, attention\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "\n",
        "    def forward_with_attention(self, x, y=None, mask=None):\n",
        "        x_, attention = self.attn(self.norm1(x), y, mask)\n",
        "        x = x + x_\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, attention\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        x = x + self.attn(self.norm1(x), y, mask)[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n",
        "                 norm_layer: nn.Module = nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim_self)\n",
        "        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n",
        "        self.norm2 = norm_layer(dim_self)\n",
        "        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def forward_with_attention(self, x, y=None, mask=None):\n",
        "        attentions = []\n",
        "        for layer in self.layers:\n",
        "            x, att = layer.forward_with_attention(x, y, mask)\n",
        "            attentions.append(att)\n",
        "        return x, attentions\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i % 2 == 0 and self.enc_dec: # cross\n",
        "                x = layer(x, y)\n",
        "            elif self.enc_dec:  # self\n",
        "                x = layer(x, x, mask)\n",
        "            else:  # self or cross\n",
        "                x = layer(x, y, mask)\n",
        "        return x\n",
        "\n",
        "    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n",
        "                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n",
        "        super(Transformer, self).__init__()\n",
        "        dim_ref = dim_ref if dim_ref is not None else dim_self\n",
        "        self.enc_dec = enc_dec\n",
        "        if enc_dec:\n",
        "            num_layers = num_layers * 2\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            if i % 2 == 0 and enc_dec:  # cross\n",
        "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "            elif enc_dec:  # self\n",
        "                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "            else:  # self or cross\n",
        "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "class TransformerMapper(nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n",
        "        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n",
        "        prefix = torch.cat((x, prefix), dim=1)\n",
        "        out = self.transformer(prefix)[:, self.clip_length:]\n",
        "        return out\n",
        "\n",
        "    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n",
        "        super(TransformerMapper, self).__init__()\n",
        "        self.clip_length = clip_length\n",
        "        self.transformer = Transformer(dim_embedding, 8, num_layers)\n",
        "        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n",
        "        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
        "                labels: Optional[torch.Tensor] = None):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
        "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if mapping_type == MappingType.MLP:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
        "                                     self.gpt_embedding_size * prefix_length))\n",
        "        else:\n",
        "            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
        "                                                                     clip_length, num_layers)\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self\n",
        "\n",
        "\n",
        "def save_config(args: argparse.Namespace):\n",
        "    config = {}\n",
        "    for key, item in args._get_kwargs():\n",
        "        config[key] = item\n",
        "    out_path = os.path.join(args.out_dir, f\"{args.prefix}.json\")\n",
        "    with open(out_path, 'w') as outfile:\n",
        "        json.dump(config, outfile)\n",
        "\n",
        "\n",
        "def load_model(config_path: str, epoch_or_latest: Union[str, int] = '_latest'):\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.set_defaults(**config)\n",
        "    args = parser.parse_args()\n",
        "    if type(epoch_or_latest) is int:\n",
        "        epoch_or_latest = f\"-{epoch_or_latest:03d}\"\n",
        "    model_path = os.path.join(args.out_dir, f\"{args.prefix}{epoch_or_latest}.pt\")\n",
        "    if args.only_prefix:\n",
        "        model = ClipCaptionPrefix(args.prefix_length)\n",
        "    else:\n",
        "        model = ClipCaptionModel(args.prefix_length)\n",
        "    if os.path.isfile(model_path):\n",
        "        print(f\"loading model from {model_path}\")\n",
        "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    else:\n",
        "        print(f\"{model_path} is not exist\")\n",
        "    return model, parser\n",
        "\n",
        "\n",
        "def train(dataset: ClipDataset, model: ClipCaptionModel, args,\n",
        "          lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
        "\n",
        "    device = torch.device('cuda:0')\n",
        "    batch_size = args.bs\n",
        "    epochs = args.epochs\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
        "    )\n",
        "    # save_config(args)\n",
        "    for epoch in range(epochs):\n",
        "        print(f\">>> Training epoch {epoch}\")\n",
        "        sys.stdout.flush()\n",
        "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
        "        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n",
        "            model.zero_grad()\n",
        "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
        "            outputs = model(tokens, prefix, mask)\n",
        "            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
        "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress.set_postfix({\"loss\": loss.item()})\n",
        "            progress.update()\n",
        "            if (idx + 1) % 10000 == 0:\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
        "                )\n",
        "        progress.close()\n",
        "        if epoch % args.save_every == 0 or epoch == epochs - 1:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
        "            )\n",
        "    return model\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data', default='split_train.pkl')\n",
        "    parser.add_argument('--out_dir', default='savepoint')\n",
        "    parser.add_argument('--prefix', default='prefix', help='prefix for saved filenames')\n",
        "    parser.add_argument('--epochs', type=int, default=10)\n",
        "    parser.add_argument('--save_every', type=int, default=1)\n",
        "    parser.add_argument('--prefix_length', type=int, default=10)\n",
        "    parser.add_argument('--prefix_length_clip', type=int, default=10)\n",
        "    parser.add_argument('--bs', type=int, default=40)\n",
        "    parser.add_argument('--only_prefix', dest='only_prefix', action='store_true')\n",
        "    parser.add_argument('--mapping_type', type=str, default='mlp', help='mlp/transformer')\n",
        "    parser.add_argument('--num_layers', type=int, default=8)\n",
        "    parser.add_argument('--is_rn', dest='is_rn', action='store_true')\n",
        "    parser.add_argument('--normalize_prefix', dest='normalize_prefix', action='store_true')\n",
        "    args = parser.parse_known_args()[0] #แก้จาก args = parser.parse_args()\n",
        "    prefix_length = args.prefix_length\n",
        "    dataset = ClipDataset(args.data, prefix_length, normalize_prefix=args.normalize_prefix)\n",
        "    prefix_dim = 640 if args.is_rn else 512\n",
        "    args.mapping_type = {'mlp': MappingType.MLP, 'transformer': MappingType.Transformer}[args.mapping_type]\n",
        "    if args.only_prefix:\n",
        "        model = ClipCaptionPrefix(prefix_length, clip_length=args.prefix_length_clip, prefix_size=prefix_dim,\n",
        "                                  num_layers=args.num_layers, mapping_type=args.mapping_type)\n",
        "        print(\"Train only prefix\")\n",
        "    else:\n",
        "        model = ClipCaptionModel(prefix_length, clip_length=args.prefix_length_clip, prefix_size=prefix_dim,\n",
        "                                  num_layers=args.num_layers, mapping_type=args.mapping_type)\n",
        "        print(\"Train both prefix and GPT\")\n",
        "        sys.stdout.flush()\n",
        "    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}